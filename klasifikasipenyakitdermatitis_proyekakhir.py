# -*- coding: utf-8 -*-
"""KlasifikasiPenyakitDermatitis_ProyekAkhir.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RkjlxFY2ml4WilhlPpLRg68aZc1cSouB
"""

from google.colab import drive
drive.mount('/content/drive')

dataset_path = '/content/drive/MyDrive/Proyek Akhir (PA)/Dataset'

"""# Resnet-50

"""

import tensorflow as tf

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical',
    validation_split=0.2,
    subset='training',
    seed=123
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical',
    validation_split=0.2,
    subset='validation',
    seed=123
)

class_names = train_ds.class_names

preprocess_input = tf.keras.applications.resnet50.preprocess_input
train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y))
val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y))

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

base_model = tf.keras.applications.ResNet50(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False

model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# 8. Train
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10
)

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt

def predict_image_class(img_path, model, class_names):
    """
    Predict the class of an input image using a trained model.

    Parameters:
    - img_path (str): Path to the image file.
    - model (tf.keras.Model): Trained Keras model.
    - class_names (list): List of class names, usually from train_ds.class_names.

    Returns:
    - predicted_label (str): Class name with the highest prediction score.
    """

    # Load and preprocess image
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = tf.keras.applications.resnet50.preprocess_input(img_array)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension

    # Predict
    predictions = model.predict(img_array)
    predicted_index = np.argmax(predictions[0])
    predicted_label = class_names[predicted_index]

    # Optionally show the image and prediction
    plt.imshow(img)
    plt.axis('off')
    plt.title(f"Prediction: {predicted_label}")
    plt.show()

    return predicted_label

from google.colab import files
uploaded = files.upload()

img_path = 'test_atopic.jpeg'
class_names = train_ds.class_names

predict_image_class(img_path, model, class_names)

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import itertools
import matplotlib.pyplot as plt

# Step 1: Get true labels and predicted labels from validation dataset
y_true = []
y_pred = []

# class_names from train_ds.class_names
for images, labels in val_ds:
    preds = model.predict(images)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

# Step 2: Generate report
print("Classification Report:\n")
print(classification_report(y_true, y_pred, target_names=class_names))

def plot_confusion_matrix(cm, class_names):
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.colorbar()

    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=45, ha='right')
    plt.yticks(tick_marks, class_names)

    thresh = cm.max() / 2.0
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.show()

# Step 3: Compute and plot
cm = confusion_matrix(y_true, y_pred)
plot_confusion_matrix(cm, class_names)

"""# Derm Foundation"""

!pip install -U jax jaxlib tensorflow tensorflow_datasets

import tensorflow as tf
import os
import random
from glob import glob
from sklearn.model_selection import train_test_split

# Set your dataset path
DATASET_DIR = '/content/drive/MyDrive/Proyek Akhir (PA)/Dataset'
OUTPUT_DIR = '/content/drive/MyDrive/Proyek Akhir (PA)/derm_tfrecords'
os.makedirs(OUTPUT_DIR, exist_ok=True)

IMG_SIZE = (224, 224)
VALIDATION_SPLIT = 0.2

# Gather image paths and labels
class_names = sorted(os.listdir(DATASET_DIR))
label_to_index = {name: idx for idx, name in enumerate(class_names)}

image_paths = []
labels = []

for class_name in class_names:
    class_dir = os.path.join(DATASET_DIR, class_name)
    for img_path in glob(os.path.join(class_dir, '*')):
        image_paths.append(img_path)
        labels.append(label_to_index[class_name])

# Split into train and val
train_paths, val_paths, train_labels, val_labels = train_test_split(
    image_paths, labels, test_size=VALIDATION_SPLIT, random_state=42, stratify=labels
)

def serialize_example(image_string, label):
    feature = {
        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_string])),
        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),
    }
    example = tf.train.Example(features=tf.train.Features(feature=feature))
    return example.SerializeToString()

def write_tfrecord(paths, labels, filename):
    with tf.io.TFRecordWriter(filename) as writer:
        for path, label in zip(paths, labels):
            img = tf.io.read_file(path)
            try:
                img = tf.image.decode_jpeg(img, channels=3)
                img = tf.image.resize(img, IMG_SIZE)
                img = tf.cast(img, tf.uint8)
                img_bytes = tf.io.encode_jpeg(tf.cast(img, tf.uint8)).numpy()
                example = serialize_example(img_bytes, label)
                writer.write(example)
            except Exception as e:
                print(f"Skipping {path}: {e}")

# Write TFRecord files
train_tfrecord = os.path.join(OUTPUT_DIR, 'train.tfrecord')
val_tfrecord = os.path.join(OUTPUT_DIR, 'val.tfrecord')

write_tfrecord(train_paths, train_labels, train_tfrecord)
write_tfrecord(val_paths, val_labels, val_tfrecord)

print("TFRecord conversion complete!")